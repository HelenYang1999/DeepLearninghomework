{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1: Multi-Layer Perceptron with MNIST Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, you are required to train two MLPs to classify images from the [MNIST database](http://yann.lecun.com/exdb/mnist/) hand-written digit database by using PyTorch.\n",
    "\n",
    "The process will be broken down into the following steps:\n",
    ">1. Load and visualize the data.\n",
    "2. Define a neural network. (30 marks)\n",
    "3. Train the models. (30 marks)\n",
    "4. Evaluate the performance of our trained models on the test dataset. (20 marks)\n",
    "5. Analysis your results. (20 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Load and Visualize the Data\n",
    "\n",
    "Downloading may take a few moments, and you should see your progress as the data is loading. You may also choose to change the `batch_size` if you want to load more data at a time.\n",
    "\n",
    "This cell will create DataLoaders for each of our datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# number of subprocesses to use for data loading\n",
    "num_workers = 8\n",
    "# how many samples per batch to load\n",
    "batch_size = 20\n",
    "\n",
    "# convert data to torch.FloatTensor\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "# choose the training and test datasets\n",
    "train_data = datasets.MNIST(root='data', train=True,\n",
    "                                   download=True, transform=transform)\n",
    "test_data = datasets.MNIST(root='data', train=False,\n",
    "                                  download=True, transform=transform)\n",
    "\n",
    "# prepare data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, num_workers=num_workers)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Define the Network Architecture (30 marks)\n",
    "\n",
    "* Input: a 784-dim Tensor of pixel values for each image.\n",
    "* Output: a 10-dim Tensor of number of classes that indicates the class scores for an input image. \n",
    "\n",
    "You need to implement three models:\n",
    "1. a vanilla multi-layer perceptron. (10 marks)\n",
    "2. a multi-layer perceptron with regularization (dropout or L2 or both). (10 marks)\n",
    "3. the corresponding loss functions and optimizers. (10 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build model_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.098675 \tTraining Acc: 28.02%%\n",
      "Epoch: 2 \tTraining Loss: 0.029445 \tTraining Acc: 81.98%%\n",
      "Epoch: 3 \tTraining Loss: 0.014186 \tTraining Acc: 92.00%%\n",
      "Epoch: 4 \tTraining Loss: 0.009195 \tTraining Acc: 94.78%%\n",
      "Epoch: 5 \tTraining Loss: 0.006911 \tTraining Acc: 96.12%%\n",
      "102.5456319\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import torch\n",
    "## Define the MLP architecture\n",
    "#“Vanilla” Neural Network : this type of MLP has only a single hidden layer  \n",
    "device = torch.device(\"cuda:0,1\")\n",
    "print(torch.device.)\n",
    "class VanillaMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VanillaMLP, self).__init__()\n",
    "        # implement your codes here\n",
    "        self.linear1 = torch.nn.Linear(784,200)\n",
    "        self.linear2 = torch.nn.Linear(200,100)\n",
    "        self.linear3 = torch.nn.Linear(100,50)\n",
    "        self.linear4 = torch.nn.Linear(50,20)\n",
    "        self.linear5 = torch.nn.Linear(20,10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # flatten image input\n",
    "        x = x.view(-1, 28 * 28)\n",
    "        h_relu1 = self.linear1(x).clamp(min=0)\n",
    "        h_relu2 = self.linear2(h_relu1).clamp(min=0)\n",
    "        h_relu3 = self.linear3(h_relu2).clamp(min=0)\n",
    "        h_relu4 = self.linear4(h_relu3).clamp(min=0)\n",
    "        y_pred = self.linear5(h_relu4)\n",
    "        # implement your codes here   \n",
    "        return F.log_softmax(y_pred,dim=1)\n",
    "\n",
    "# initialize the MLP\n",
    "\n",
    "model_1 = VanillaMLP()\n",
    "model_1.to(device)\n",
    "# specify loss function\n",
    "# implement your codes here\n",
    "def loss_function(y_pred,y):\n",
    "    loss = F.nll_loss(y_pred,y)\n",
    "    return loss\n",
    "\n",
    "# specify your optimizer\n",
    "# implement your codes here\n",
    "#随机梯度下降优化\n",
    "optimizer = torch.optim.SGD(model_1.parameters(),lr=1e-2)\n",
    "\n",
    "n_epochs = 5  # suggest training between 20-50 epochs\n",
    "# model_1 = model_1.cuda()\n",
    "model_1.train() # prep model for training\n",
    "\n",
    "start = time.clock()\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # monitor training loss\n",
    "    train_loss = 0.0\n",
    "    total_correct = 0\n",
    "    \n",
    "    for data, target in train_loader:\n",
    "        data = data.to(device)\n",
    "#         data = data.gpu()\n",
    "#         data = data.cuda()\n",
    "        target = target.to(device)\n",
    "#         target = target.gpu()\n",
    "#         target = target.cuda()\n",
    "        y_pred = model_1(data)\n",
    "        loss = loss_function(y_pred,target)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # implement your code here\n",
    "        # the total loss of this batch\n",
    "        train_loss += loss.item()\n",
    "        # the accumulated number of correctly classified samples of this batch\n",
    "        _,pred = torch.max(y_pred.data,1)\n",
    "        total_correct += torch.sum(pred == target.data).item()\n",
    "        \n",
    "    # print training statistics \n",
    "    # calculate average loss and accuracy over an epoch\n",
    "    train_loss = train_loss / len(train_loader.dataset)\n",
    "    train_acc = 100. * total_correct / len(train_loader.dataset)\n",
    "    \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tTraining Acc: {:.2f}%%'.format(\n",
    "        epoch+1, \n",
    "        train_loss,\n",
    "        train_acc\n",
    "        ))\n",
    "end = time.clock()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Train the Network (30 marks)\n",
    "\n",
    "Train your models in the following two cells.\n",
    "\n",
    "The following loop trains for 30 epochs; feel free to change this number. For now, we suggest somewhere between 20-50 epochs. As you train, take a look at how the values for the training loss decrease over time. We want it to decrease while also avoiding overfitting the training data. \n",
    "\n",
    "**The key parts in the training process are left for you to implement.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.095409 \tTraining Acc: 34.72%%\n",
      "Epoch: 2 \tTraining Loss: 0.025924 \tTraining Acc: 84.44%%\n",
      "Epoch: 3 \tTraining Loss: 0.015137 \tTraining Acc: 91.43%%\n",
      "Epoch: 4 \tTraining Loss: 0.009951 \tTraining Acc: 94.26%%\n",
      "Epoch: 5 \tTraining Loss: 0.007405 \tTraining Acc: 95.74%%\n",
      "40.38718510000001\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import torch\n",
    "## Define the MLP architecture\n",
    "#“Vanilla” Neural Network : this type of MLP has only a single hidden layer \n",
    "device = torch.device(\"cpu\") \n",
    "\n",
    "class VanillaMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VanillaMLP, self).__init__()\n",
    "        # implement your codes here\n",
    "        self.linear1 = torch.nn.Linear(784,200)\n",
    "        self.linear2 = torch.nn.Linear(200,100)\n",
    "        self.linear3 = torch.nn.Linear(100,50)\n",
    "        self.linear4 = torch.nn.Linear(50,20)\n",
    "        self.linear5 = torch.nn.Linear(20,10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # flatten image input\n",
    "        x = x.view(-1, 28 * 28)\n",
    "        h_relu1 = self.linear1(x).clamp(min=0)\n",
    "        h_relu2 = self.linear2(h_relu1).clamp(min=0)\n",
    "        h_relu3 = self.linear3(h_relu2).clamp(min=0)\n",
    "        h_relu4 = self.linear4(h_relu3).clamp(min=0)\n",
    "        y_pred = self.linear5(h_relu4)\n",
    "        # implement your codes here   \n",
    "        return F.log_softmax(y_pred,dim=1)\n",
    "\n",
    "# initialize the MLP\n",
    "\n",
    "model_1 = VanillaMLP()\n",
    "model_1.to(device)\n",
    "# specify loss function\n",
    "# implement your codes here\n",
    "def loss_function(y_pred,y):\n",
    "    loss = F.nll_loss(y_pred,y)\n",
    "    return loss\n",
    "\n",
    "# specify your optimizer\n",
    "# implement your codes here\n",
    "#随机梯度下降优化\n",
    "optimizer = torch.optim.SGD(model_1.parameters(),lr=1e-2)\n",
    "\n",
    "n_epochs = 5  # suggest training between 20-50 epochs\n",
    "# model_1 = model_1.cuda()\n",
    "model_1.train() # prep model for training\n",
    "\n",
    "start = time.clock()\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # monitor training loss\n",
    "    train_loss = 0.0\n",
    "    total_correct = 0\n",
    "    \n",
    "    for data, target in train_loader:\n",
    "        data = data.to(device)\n",
    "#         data = data.gpu()\n",
    "#         data = data.cuda()\n",
    "        target = target.to(device)\n",
    "#         target = target.gpu()\n",
    "#         target = target.cuda()\n",
    "        y_pred = model_1(data)\n",
    "        loss = loss_function(y_pred,target)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # implement your code here\n",
    "        # the total loss of this batch\n",
    "        train_loss += loss.item()\n",
    "        # the accumulated number of correctly classified samples of this batch\n",
    "        _,pred = torch.max(y_pred.data,1)\n",
    "        total_correct += torch.sum(pred == target.data).item()\n",
    "        \n",
    "    # print training statistics \n",
    "    # calculate average loss and accuracy over an epoch\n",
    "    train_loss = train_loss / len(train_loader.dataset)\n",
    "    train_acc = 100. * total_correct / len(train_loader.dataset)\n",
    "    \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tTraining Acc: {:.2f}%%'.format(\n",
    "        epoch+1, \n",
    "        train_loss,\n",
    "        train_acc\n",
    "        ))\n",
    "end = time.clock()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
