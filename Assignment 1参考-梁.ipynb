{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1: Multi-Layer Perceptron with MNIST Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, you are required to train two MLPs to classify images from the [MNIST database](http://yann.lecun.com/exdb/mnist/) hand-written digit database by using PyTorch.\n",
    "\n",
    "The process will be broken down into the following steps:\n",
    ">1. Load and visualize the data.\n",
    "2. Define a neural network. (30 marks)\n",
    "3. Train the models. (30 marks)\n",
    "4. Evaluate the performance of our trained models on the test dataset. (20 marks)\n",
    "5. Analysis your results. (20 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Load and Visualize the Data\n",
    "\n",
    "Downloading may take a few moments, and you should see your progress as the data is loading. You may also choose to change the `batch_size` if you want to load more data at a time.\n",
    "\n",
    "This cell will create DataLoaders for each of our datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# number of subprocesses to use for data loading\n",
    "num_workers = 0\n",
    "# how many samples per batch to load\n",
    "batch_size = 20\n",
    "\n",
    "# convert data to torch.FloatTensor\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "# choose the training and test datasets\n",
    "train_data = datasets.MNIST(root='data', train=True,\n",
    "                                   download=True, transform=transform)\n",
    "test_data = datasets.MNIST(root='data', train=False,\n",
    "                                  download=True, transform=transform)\n",
    "\n",
    "# prepare data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, num_workers=num_workers)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset MNIST\n",
       "    Number of datapoints: 10000\n",
       "    Root location: data\n",
       "    Split: Test\n",
       "    StandardTransform\n",
       "Transform: ToTensor()"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize a Batch of Training Data\n",
    "\n",
    "The first step in a classification task is to take a look at the data, make sure it is loaded in correctly, then make any initial observations about patterns in that data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "    \n",
    "# obtain one batch of training images\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = dataiter.next()\n",
    "images = images.numpy()\n",
    "\n",
    "# plot the images in the batch, along with the corresponding labels\n",
    "fig = plt.figure(figsize=(25, 4))\n",
    "for idx in np.arange(20):\n",
    "    ax = fig.add_subplot(2, 20/2, idx+1, xticks=[], yticks=[])\n",
    "    ax.imshow(np.squeeze(images[idx]), cmap='gray')\n",
    "    # print out the correct label for each image\n",
    "    # .item() gets the value contained in a Tensor\n",
    "    ax.set_title(str(labels[idx].item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View an Image in More Detail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = np.squeeze(images[1])\n",
    "\n",
    "fig = plt.figure(figsize = (12,12)) \n",
    "ax = fig.add_subplot(111)\n",
    "ax.imshow(img, cmap='gray')\n",
    "width, height = img.shape\n",
    "thresh = img.max()/2.5\n",
    "for x in range(width):\n",
    "    for y in range(height):\n",
    "        val = round(img[x][y],2) if img[x][y] !=0 else 0\n",
    "        ax.annotate(str(val), xy=(y,x),\n",
    "                    horizontalalignment='center',\n",
    "                    verticalalignment='center',\n",
    "                    color='white' if img[x][y]<thresh else 'black')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Define the Network Architecture (30 marks)\n",
    "\n",
    "* Input: a 784-dim Tensor of pixel values for each image.\n",
    "* Output: a 10-dim Tensor of number of classes that indicates the class scores for an input image. \n",
    "\n",
    "You need to implement three models:\n",
    "1. a vanilla multi-layer perceptron. (10 marks)\n",
    "2. a multi-layer perceptron with regularization (dropout or L2 or both). (10 marks)\n",
    "3. the corresponding loss functions and optimizers. (10 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build model_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define the MLP architecture\n",
    "import torch.nn as nn\n",
    "class VanillaMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VanillaMLP, self).__init__()\n",
    "        \n",
    "        # implement your codes here\n",
    "        num_inputs, num_outputs, num_hiddens = 784, 10, [64, 128, 256] \n",
    "        self.l1 = nn.Linear(num_inputs, num_hiddens[0])\n",
    "        self.l2 = nn.Linear(num_hiddens[0], num_hiddens[1])\n",
    "        self.l3 = nn.Linear(num_hiddens[1], num_hiddens[2])\n",
    "        self.relu = nn.ReLU()\n",
    "        self.l4 = nn.Linear(num_hiddens[2], num_outputs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # flatten image input\n",
    "        x = x.view(-1, 28 * 28)\n",
    "\n",
    "        # implement your codes here\n",
    "        x = self.relu(self.l1(x))\n",
    "        x = self.relu(self.l2(x))\n",
    "        x = self.relu(self.l3(x))\n",
    "        x = self.l4(x)\n",
    "        return x\n",
    "\n",
    "# initialize the MLP\n",
    "model_1 = VanillaMLP()\n",
    "model_1.to(device)\n",
    "\n",
    "# specify loss function\n",
    "# implement your codes here\n",
    "loss_1 = torch.nn.CrossEntropyLoss()\n",
    "loss_1.to(device)\n",
    "# specify your optimizer\n",
    "# implement your codes here\n",
    "optimizer_1 = torch.optim.SGD(model_1.parameters(), lr = 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build model_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RegularizedMLP(\n",
      "  (l1): Linear(in_features=784, out_features=64, bias=True)\n",
      "  (l2): Linear(in_features=64, out_features=128, bias=True)\n",
      "  (l3): Linear(in_features=128, out_features=256, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (dropout1): Dropout(p=0.2, inplace=False)\n",
      "  (dropout2): Dropout(p=0.3, inplace=False)\n",
      "  (dropout3): Dropout(p=0.5, inplace=False)\n",
      "  (l4): Linear(in_features=256, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "## Define the MLP architecture\n",
    "class RegularizedMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RegularizedMLP, self).__init__()\n",
    "        \n",
    "        # implement your codes here\n",
    "        num_inputs, num_outputs, num_hiddens = 784, 10, [64, 128, 256] \n",
    "        drop_prob = [0.2, 0.3, 0.5]\n",
    "        self.l1 = nn.Linear(num_inputs, num_hiddens[0])\n",
    "        self.l2 = nn.Linear(num_hiddens[0], num_hiddens[1])\n",
    "        self.l3 = nn.Linear(num_hiddens[1], num_hiddens[2])\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(drop_prob[0])\n",
    "        self.dropout2 = nn.Dropout(drop_prob[1])\n",
    "        self.dropout3 = nn.Dropout(drop_prob[2])\n",
    "        self.l4 = nn.Linear(num_hiddens[2], num_outputs)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # flatten image input\n",
    "        x = x.view(-1, 28 * 28)\n",
    "\n",
    "        # implement your codes here\n",
    "        x = self.dropout1(self.relu(self.l1(x)))\n",
    "        x = self.dropout2(self.relu(self.l2(x)))\n",
    "        x = self.dropout3(self.relu(self.l3(x)))\n",
    "        x = self.l4(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# initialize the MLP\n",
    "model_2 = RegularizedMLP()\n",
    "model_2.to(device)\n",
    "print(model_2)\n",
    "# specify loss function\n",
    "# implement your codes here\n",
    "loss_2 = torch.nn.CrossEntropyLoss()\n",
    "loss_2.to(device)\n",
    "\n",
    "# specify your optimizer\n",
    "# implement your codes here\n",
    "weight_decay = 0.001\n",
    "weight_decay_list = (param for name, param in model_2.named_parameters() if name[-4:] != 'bias' and \"bn\" not in name)\n",
    "no_decay_list = (param for name, param in model_2.named_parameters() if name[-4:] == 'bias' or \"bn\" in name)\n",
    "\n",
    "parameters = [{'params': weight_decay_list},\n",
    "              {'params': no_decay_list, 'weight_decay': 0.}]\n",
    "\n",
    "optimizer_2 = torch.optim.SGD(parameters, weight_decay=weight_decay, lr = 0.1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Train the Network (30 marks)\n",
    "\n",
    "Train your models in the following two cells.\n",
    "\n",
    "The following loop trains for 30 epochs; feel free to change this number. For now, we suggest somewhere between 20-50 epochs. As you train, take a look at how the values for the training loss decrease over time. We want it to decrease while also avoiding overfitting the training data. \n",
    "\n",
    "**The key parts in the training process are left for you to implement.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.017278 \tTraining Acc: 89.17%\n",
      "Epoch: 2 \tTraining Loss: 0.006289 \tTraining Acc: 96.17%\n",
      "Epoch: 3 \tTraining Loss: 0.004551 \tTraining Acc: 97.29%\n",
      "Epoch: 4 \tTraining Loss: 0.003568 \tTraining Acc: 97.88%\n",
      "Epoch: 5 \tTraining Loss: 0.002905 \tTraining Acc: 98.25%\n",
      "Epoch: 6 \tTraining Loss: 0.002445 \tTraining Acc: 98.51%\n",
      "Epoch: 7 \tTraining Loss: 0.002214 \tTraining Acc: 98.69%\n",
      "Epoch: 8 \tTraining Loss: 0.001848 \tTraining Acc: 98.89%\n",
      "Epoch: 9 \tTraining Loss: 0.001562 \tTraining Acc: 98.96%\n",
      "Epoch: 10 \tTraining Loss: 0.001407 \tTraining Acc: 99.11%\n",
      "Epoch: 11 \tTraining Loss: 0.001348 \tTraining Acc: 99.10%\n",
      "Epoch: 12 \tTraining Loss: 0.001347 \tTraining Acc: 99.07%\n",
      "Epoch: 13 \tTraining Loss: 0.001220 \tTraining Acc: 99.20%\n",
      "Epoch: 14 \tTraining Loss: 0.000990 \tTraining Acc: 99.32%\n",
      "Epoch: 15 \tTraining Loss: 0.001075 \tTraining Acc: 99.27%\n",
      "Epoch: 16 \tTraining Loss: 0.000827 \tTraining Acc: 99.45%\n",
      "Epoch: 17 \tTraining Loss: 0.000980 \tTraining Acc: 99.34%\n",
      "Epoch: 18 \tTraining Loss: 0.000762 \tTraining Acc: 99.52%\n",
      "Epoch: 19 \tTraining Loss: 0.000721 \tTraining Acc: 99.53%\n",
      "Epoch: 20 \tTraining Loss: 0.000825 \tTraining Acc: 99.48%\n",
      "Epoch: 21 \tTraining Loss: 0.000808 \tTraining Acc: 99.52%\n",
      "Epoch: 22 \tTraining Loss: 0.000544 \tTraining Acc: 99.60%\n",
      "Epoch: 23 \tTraining Loss: 0.000704 \tTraining Acc: 99.49%\n",
      "Epoch: 24 \tTraining Loss: 0.000665 \tTraining Acc: 99.58%\n",
      "Epoch: 25 \tTraining Loss: 0.000773 \tTraining Acc: 99.48%\n",
      "Epoch: 26 \tTraining Loss: 0.000399 \tTraining Acc: 99.76%\n",
      "Epoch: 27 \tTraining Loss: 0.000524 \tTraining Acc: 99.62%\n",
      "Epoch: 28 \tTraining Loss: 0.000613 \tTraining Acc: 99.61%\n",
      "Epoch: 29 \tTraining Loss: 0.000410 \tTraining Acc: 99.73%\n",
      "Epoch: 30 \tTraining Loss: 0.000649 \tTraining Acc: 99.61%\n"
     ]
    }
   ],
   "source": [
    "# number of epochs to train the model\n",
    "n_epochs = 30  # suggest training between 20-50 epochs\n",
    "model_1.train() # prep model for training\n",
    "\n",
    "def accuracy(y_hat, y):\n",
    "    #print((y_hat.argmax(dim=1)== y).float().sum())\n",
    "    return (y_hat.argmax(dim=1) == y).float().sum().item()\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # monitor training loss\n",
    "    train_loss = 0.0\n",
    "    total_correct = 0\n",
    "    \n",
    "    for data, target in train_loader:\n",
    "        # implement your code here\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        predicts = model_1(data)\n",
    "        #print(predicts, target)\n",
    "        l = loss_1(predicts, target).sum()\n",
    "       \n",
    "        optimizer_1.zero_grad()\n",
    "        l.backward()\n",
    "        optimizer_1.step()\n",
    "        \n",
    "        train_loss += l # the total loss of this batch\n",
    "        total_correct += accuracy(predicts, target) # the accumulated number of correctly classified samples of this batch\n",
    "        \n",
    "    # print training statistics \n",
    "    # calculate average loss and accuracy over an epoch\n",
    "    train_loss = train_loss / len(train_loader.dataset)\n",
    "    train_acc = 100. * total_correct / len(train_loader.dataset)\n",
    "    \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tTraining Acc: {:.2f}%'.format(\n",
    "        epoch+1, \n",
    "        train_loss,\n",
    "        train_acc\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.024614 \tTraining Acc: 84.87%\n",
      "Epoch: 2 \tTraining Loss: 0.012468 \tTraining Acc: 92.77%\n",
      "Epoch: 3 \tTraining Loss: 0.010735 \tTraining Acc: 93.72%\n",
      "Epoch: 4 \tTraining Loss: 0.010144 \tTraining Acc: 94.14%\n",
      "Epoch: 5 \tTraining Loss: 0.009851 \tTraining Acc: 94.25%\n",
      "Epoch: 6 \tTraining Loss: 0.009311 \tTraining Acc: 94.57%\n",
      "Epoch: 7 \tTraining Loss: 0.008983 \tTraining Acc: 94.82%\n",
      "Epoch: 8 \tTraining Loss: 0.009018 \tTraining Acc: 94.73%\n",
      "Epoch: 9 \tTraining Loss: 0.008721 \tTraining Acc: 94.90%\n",
      "Epoch: 10 \tTraining Loss: 0.008652 \tTraining Acc: 94.90%\n",
      "Epoch: 11 \tTraining Loss: 0.008468 \tTraining Acc: 95.10%\n",
      "Epoch: 12 \tTraining Loss: 0.008502 \tTraining Acc: 94.97%\n",
      "Epoch: 13 \tTraining Loss: 0.008481 \tTraining Acc: 95.05%\n",
      "Epoch: 14 \tTraining Loss: 0.008370 \tTraining Acc: 95.03%\n",
      "Epoch: 15 \tTraining Loss: 0.008474 \tTraining Acc: 95.10%\n",
      "Epoch: 16 \tTraining Loss: 0.008266 \tTraining Acc: 95.11%\n",
      "Epoch: 17 \tTraining Loss: 0.008420 \tTraining Acc: 95.10%\n",
      "Epoch: 18 \tTraining Loss: 0.008320 \tTraining Acc: 95.29%\n",
      "Epoch: 19 \tTraining Loss: 0.008258 \tTraining Acc: 95.21%\n",
      "Epoch: 20 \tTraining Loss: 0.008076 \tTraining Acc: 95.28%\n",
      "Epoch: 21 \tTraining Loss: 0.008237 \tTraining Acc: 95.05%\n",
      "Epoch: 22 \tTraining Loss: 0.008188 \tTraining Acc: 95.23%\n",
      "Epoch: 23 \tTraining Loss: 0.008307 \tTraining Acc: 95.20%\n",
      "Epoch: 24 \tTraining Loss: 0.008217 \tTraining Acc: 95.22%\n",
      "Epoch: 25 \tTraining Loss: 0.008009 \tTraining Acc: 95.30%\n",
      "Epoch: 26 \tTraining Loss: 0.008173 \tTraining Acc: 95.26%\n",
      "Epoch: 27 \tTraining Loss: 0.008114 \tTraining Acc: 95.24%\n",
      "Epoch: 28 \tTraining Loss: 0.008296 \tTraining Acc: 95.12%\n",
      "Epoch: 29 \tTraining Loss: 0.008236 \tTraining Acc: 95.20%\n",
      "Epoch: 30 \tTraining Loss: 0.008046 \tTraining Acc: 95.29%\n"
     ]
    }
   ],
   "source": [
    "# number of epochs to train the model\n",
    "n_epochs = 30  # suggest training between 20-50 epochs\n",
    "\n",
    "model_2.train() # prep model for training\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # monitor training loss\n",
    "    train_loss = 0.0\n",
    "    total_correct = 0\n",
    "    \n",
    "    for data, target in train_loader:\n",
    "        # implement your code here\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        predicts = model_2(data)\n",
    "        l = loss_2(predicts, target).sum()\n",
    "       \n",
    "        optimizer_2.zero_grad()\n",
    "        l.backward()\n",
    "        optimizer_2.step()\n",
    "        \n",
    "        train_loss += l # the total loss of this batch\n",
    "        total_correct += accuracy(predicts, target) # the accumulated number of correctly classified samples of this batch\n",
    "        \n",
    "        \n",
    "    # print training statistics \n",
    "    # calculate average loss and accuracy over an epoch\n",
    "    train_loss = train_loss / len(train_loader.dataset)\n",
    "    train_acc = 100. * total_correct / len(train_loader.dataset)\n",
    "    \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tTraining Acc: {:.2f}%'.format(\n",
    "        epoch+1, \n",
    "        train_loss,\n",
    "        train_acc\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Test the Trained Network (20 marks)\n",
    "\n",
    "Test the performance of trained models on test data. Except the total test accuracy, you should calculate the accuracy for each class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test model_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.000000\n",
      "\n",
      "Test Accuracy of class 0: 100.00%\n",
      "Test Accuracy of class 1: N/A (no training examples)\n",
      "Test Accuracy of class 2: N/A (no training examples)\n",
      "Test Accuracy of class 3: N/A (no training examples)\n",
      "Test Accuracy of class 4: N/A (no training examples)\n",
      "Test Accuracy of class 5: N/A (no training examples)\n",
      "Test Accuracy of class 6: N/A (no training examples)\n",
      "Test Accuracy of class 7: N/A (no training examples)\n",
      "Test Accuracy of class 8: N/A (no training examples)\n",
      "Test Accuracy of class 9: N/A (no training examples)\n",
      "\n",
      "Test Accuracy (Overall): 100.00%\n"
     ]
    }
   ],
   "source": [
    "# initialize lists to monitor test loss and accuracy\n",
    "test_loss = 0.0\n",
    "class_correct = list(0. for i in range(10))\n",
    "class_total = list(0. for i in range(10))\n",
    "\n",
    "model_1.eval() # prep model for *evaluation*\n",
    "\n",
    "for data, target in test_loader:\n",
    "    # implement your code here\n",
    "    data, target = data.to(device), target.to(device)\n",
    "    preds = model_1(data)\n",
    "    test_loss = loss_1(preds, target) # the total loss of this batch\n",
    "    \n",
    "    for label in range(10):\n",
    "        class_correct[label] = accuracy(preds, target) # the list of number of correctly classified samples of each class of this batch. label is the index.\n",
    "        class_total[label] = len(data) # the list of total number of samples of each class of this batch. label is the index.\n",
    "\n",
    "\n",
    "# calculate and print avg test loss\n",
    "test_loss = test_loss / len(test_loader.dataset)\n",
    "print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
    "\n",
    "for i in range(10):\n",
    "    if class_total[i] > 0:\n",
    "        print('Test Accuracy of class %d: %.2f%%' % (i, 100 * class_correct[i] / class_total[i]))\n",
    "    else:\n",
    "        print('Test Accuracy of class %d: N/A (no training examples)' % (i))\n",
    "\n",
    "print('\\nTest Accuracy (Overall): %.2f%%' % (100. * np.sum(class_correct) / np.sum(class_total)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test model_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize lists to monitor test loss and accuracy\n",
    "test_loss = 0.0\n",
    "class_correct = list(0. for i in range(10))\n",
    "class_total = list(0. for i in range(10))\n",
    "\n",
    "model_2.eval() # prep model for *evaluation*\n",
    "\n",
    "for data, target in test_loader:\n",
    "    \n",
    "    # implement your code here\n",
    "    \n",
    "    test_loss = # the total loss of this batch\n",
    "    class_correct[label] = # the list of number of correctly classified samples of each class of this batch. label is the index.\n",
    "    class_total[label] = # the list of total number of samples of each class of this batch. label is the index.\n",
    "\n",
    "# calculate and print avg test loss\n",
    "test_loss = test_loss / len(test_loader.dataset)\n",
    "print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
    "\n",
    "for i in range(10):\n",
    "    if class_total[i] > 0:\n",
    "        print('Test Accuracy of class %d: %.2f%%' % (i, 100 * class_correct[i] / class_total[i]))\n",
    "    else:\n",
    "        print('Test Accuracy of class %d: N/A (no training examples)' % (i))\n",
    "\n",
    "print('\\nTest Accuracy (Overall): %.2f%%' % (100. * np.sum(class_correct) / np.sum(class_total)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Analyze Your Result (20 marks)\n",
    "Compare the performance of your models with the following analysis. Both English and Chinese answers are acceptable.\n",
    "1. Does your vanilla MLP overfit to the training data? (5 marks)\n",
    "\n",
    "Answer:\n",
    "\n",
    "2. If yes, how do you observe it? If no, why? (5 marks)\n",
    "\n",
    "Answer:\n",
    "\n",
    "3. Is regularized model help prevent overfitting? (5 marks)\n",
    "\n",
    "Answer:\n",
    "\n",
    "4. Generally compare the performance of two models. (5 marks)\n",
    "\n",
    "Answer:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
