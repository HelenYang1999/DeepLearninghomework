{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1: Multi-Layer Perceptron with MNIST Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, you are required to train two MLPs to classify images from the [MNIST database](http://yann.lecun.com/exdb/mnist/) hand-written digit database by using PyTorch.\n",
    "\n",
    "The process will be broken down into the following steps:\n",
    ">1. Load and visualize the data.\n",
    "2. Define a neural network. (30 marks)\n",
    "3. Train the models. (30 marks)\n",
    "4. Evaluate the performance of our trained models on the test dataset. (20 marks)\n",
    "5. Analysis your results. (20 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Load and Visualize the Data\n",
    "\n",
    "Downloading may take a few moments, and you should see your progress as the data is loading. You may also choose to change the `batch_size` if you want to load more data at a time.\n",
    "\n",
    "This cell will create DataLoaders for each of our datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# number of subprocesses to use for data loading\n",
    "num_workers = 0\n",
    "# how many samples per batch to load\n",
    "batch_size = 20\n",
    "\n",
    "# convert data to torch.FloatTensor\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "# choose the training and test datasets\n",
    "train_data = datasets.MNIST(root='data', train=True,\n",
    "                                   download=True, transform=transform)\n",
    "test_data = datasets.MNIST(root='data', train=False,\n",
    "                                  download=True, transform=transform)\n",
    "\n",
    "# prepare data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, num_workers=num_workers)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Define the Network Architecture (30 marks)\n",
    "\n",
    "* Input: a 784-dim Tensor of pixel values for each image.\n",
    "* Output: a 10-dim Tensor of number of classes that indicates the class scores for an input image. \n",
    "\n",
    "You need to implement three models:\n",
    "1. a vanilla multi-layer perceptron. (10 marks)\n",
    "2. a multi-layer perceptron with regularization (dropout or L2 or both). (10 marks)\n",
    "3. the corresponding loss functions and optimizers. (10 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build model_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import torch\n",
    "## Define the MLP architecture\n",
    "#“Vanilla” Neural Network : this type of MLP has only a single hidden layer \n",
    "device = torch.\n",
    "class VanillaMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VanillaMLP, self).__init__()\n",
    "        # implement your codes here\n",
    "        H = 64\n",
    "        self.linear1 = torch.nn.Linear(784,H)\n",
    "        self.linear2 = torch.nn.Linear(H,10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # flatten image input\n",
    "        x = x.view(-1, 28 * 28)\n",
    "        h_relu = self.linear1(x).clamp(min=0)\n",
    "        y_pred = self.linear2(h_relu)\n",
    "        # implement your codes here   \n",
    "        return F.log_softmax(y_pred,dim=1)\n",
    "\n",
    "# initialize the MLP\n",
    "\n",
    "model_1 = VanillaMLP()\n",
    "\n",
    "# specify loss function\n",
    "# implement your codes here\n",
    "def loss_function(y_pred,y):\n",
    "    loss = F.nll_loss(y_pred,y)\n",
    "    return loss\n",
    "\n",
    "# specify your optimizer\n",
    "# implement your codes here\n",
    "#随机梯度下降优化\n",
    "optimizer = torch.optim.SGD(model_1.parameters(),lr=1e-2)\n",
    "\n",
    "n_epochs = 30  # suggest training between 20-50 epochs\n",
    "# device= torch.cuda(\"cpu\") \n",
    "# model_1.to(device)\n",
    "# model_1 = model_1.cuda()\n",
    "model_1.train() # prep model for training\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # monitor training loss\n",
    "    train_loss = 0.0\n",
    "    total_correct = 0\n",
    "    \n",
    "    for data, target in train_loader:\n",
    "#         data = data.to(device)\n",
    "#         target = target.to(device)\n",
    "#         data = data.cuda()\n",
    "#         target = target.cuda()\n",
    "        y_pred = model_1(data)\n",
    "        loss = loss_function(y_pred,target)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # implement your code here\n",
    "        # the total loss of this batch\n",
    "        train_loss += loss.item()\n",
    "        # the accumulated number of correctly classified samples of this batch\n",
    "        _,pred = torch.max(y_pred.data,1)\n",
    "        total_correct += torch.sum(pred == target.data).item()\n",
    "        \n",
    "    # print training statistics \n",
    "    # calculate average loss and accuracy over an epoch\n",
    "    train_loss = train_loss / len(train_loader.dataset)\n",
    "    train_acc = 100. * total_correct / len(train_loader.dataset)\n",
    "    \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tTraining Acc: {:.2f}%%'.format(\n",
    "        epoch+1, \n",
    "        train_loss,\n",
    "        train_acc\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Train the Network (30 marks)\n",
    "\n",
    "Train your models in the following two cells.\n",
    "\n",
    "The following loop trains for 30 epochs; feel free to change this number. For now, we suggest somewhere between 20-50 epochs. As you train, take a look at how the values for the training loss decrease over time. We want it to decrease while also avoiding overfitting the training data. \n",
    "\n",
    "**The key parts in the training process are left for you to implement.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.035669 \tTraining Acc: 82.47%%\n",
      "Epoch: 2 \tTraining Loss: 0.016511 \tTraining Acc: 90.72%%\n",
      "Epoch: 3 \tTraining Loss: 0.014309 \tTraining Acc: 91.94%%\n",
      "Epoch: 4 \tTraining Loss: 0.012896 \tTraining Acc: 92.75%%\n",
      "Epoch: 5 \tTraining Loss: 0.011753 \tTraining Acc: 93.40%%\n",
      "Epoch: 6 \tTraining Loss: 0.010799 \tTraining Acc: 93.94%%\n",
      "Epoch: 7 \tTraining Loss: 0.009991 \tTraining Acc: 94.33%%\n",
      "Epoch: 8 \tTraining Loss: 0.009291 \tTraining Acc: 94.70%%\n",
      "Epoch: 9 \tTraining Loss: 0.008683 \tTraining Acc: 95.08%%\n",
      "Epoch: 10 \tTraining Loss: 0.008147 \tTraining Acc: 95.42%%\n",
      "Epoch: 11 \tTraining Loss: 0.007669 \tTraining Acc: 95.67%%\n",
      "Epoch: 12 \tTraining Loss: 0.007240 \tTraining Acc: 95.92%%\n",
      "Epoch: 13 \tTraining Loss: 0.006847 \tTraining Acc: 96.15%%\n",
      "Epoch: 14 \tTraining Loss: 0.006491 \tTraining Acc: 96.35%%\n",
      "Epoch: 15 \tTraining Loss: 0.006167 \tTraining Acc: 96.52%%\n",
      "Epoch: 16 \tTraining Loss: 0.005870 \tTraining Acc: 96.68%%\n",
      "Epoch: 17 \tTraining Loss: 0.005599 \tTraining Acc: 96.87%%\n",
      "Epoch: 18 \tTraining Loss: 0.005351 \tTraining Acc: 97.03%%\n",
      "Epoch: 19 \tTraining Loss: 0.005122 \tTraining Acc: 97.17%%\n",
      "Epoch: 20 \tTraining Loss: 0.004910 \tTraining Acc: 97.28%%\n",
      "Epoch: 21 \tTraining Loss: 0.004717 \tTraining Acc: 97.39%%\n",
      "Epoch: 22 \tTraining Loss: 0.004537 \tTraining Acc: 97.49%%\n",
      "Epoch: 23 \tTraining Loss: 0.004372 \tTraining Acc: 97.56%%\n",
      "Epoch: 24 \tTraining Loss: 0.004218 \tTraining Acc: 97.64%%\n",
      "Epoch: 25 \tTraining Loss: 0.004075 \tTraining Acc: 97.72%%\n",
      "Epoch: 26 \tTraining Loss: 0.003941 \tTraining Acc: 97.81%%\n",
      "Epoch: 27 \tTraining Loss: 0.003816 \tTraining Acc: 97.88%%\n",
      "Epoch: 28 \tTraining Loss: 0.003698 \tTraining Acc: 97.96%%\n",
      "Epoch: 29 \tTraining Loss: 0.003586 \tTraining Acc: 98.06%%\n",
      "Epoch: 30 \tTraining Loss: 0.003480 \tTraining Acc: 98.12%%\n"
     ]
    }
   ],
   "source": [
    "# number of epochs to train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.045496 \tTraining Acc: 74.35%%\n",
      "Epoch: 2 \tTraining Loss: 0.024550 \tTraining Acc: 85.99%%\n",
      "Epoch: 3 \tTraining Loss: 0.020899 \tTraining Acc: 88.01%%\n",
      "Epoch: 4 \tTraining Loss: 0.018911 \tTraining Acc: 89.09%%\n",
      "Epoch: 5 \tTraining Loss: 0.017607 \tTraining Acc: 89.76%%\n",
      "Epoch: 6 \tTraining Loss: 0.016677 \tTraining Acc: 90.45%%\n",
      "Epoch: 7 \tTraining Loss: 0.016066 \tTraining Acc: 90.67%%\n",
      "Epoch: 8 \tTraining Loss: 0.015407 \tTraining Acc: 91.06%%\n",
      "Epoch: 9 \tTraining Loss: 0.014900 \tTraining Acc: 91.25%%\n",
      "Epoch: 10 \tTraining Loss: 0.014397 \tTraining Acc: 91.58%%\n",
      "Epoch: 11 \tTraining Loss: 0.014067 \tTraining Acc: 91.78%%\n",
      "Epoch: 12 \tTraining Loss: 0.013784 \tTraining Acc: 91.98%%\n",
      "Epoch: 13 \tTraining Loss: 0.013410 \tTraining Acc: 92.12%%\n",
      "Epoch: 14 \tTraining Loss: 0.013272 \tTraining Acc: 92.25%%\n",
      "Epoch: 15 \tTraining Loss: 0.012963 \tTraining Acc: 92.34%%\n",
      "Epoch: 16 \tTraining Loss: 0.012738 \tTraining Acc: 92.55%%\n",
      "Epoch: 17 \tTraining Loss: 0.012484 \tTraining Acc: 92.55%%\n",
      "Epoch: 18 \tTraining Loss: 0.012265 \tTraining Acc: 92.80%%\n",
      "Epoch: 19 \tTraining Loss: 0.012216 \tTraining Acc: 92.71%%\n",
      "Epoch: 20 \tTraining Loss: 0.011990 \tTraining Acc: 92.97%%\n",
      "Epoch: 21 \tTraining Loss: 0.011860 \tTraining Acc: 92.89%%\n",
      "Epoch: 22 \tTraining Loss: 0.011650 \tTraining Acc: 93.17%%\n",
      "Epoch: 23 \tTraining Loss: 0.011635 \tTraining Acc: 93.02%%\n",
      "Epoch: 24 \tTraining Loss: 0.011550 \tTraining Acc: 93.01%%\n",
      "Epoch: 25 \tTraining Loss: 0.011429 \tTraining Acc: 93.17%%\n",
      "Epoch: 26 \tTraining Loss: 0.011326 \tTraining Acc: 93.14%%\n",
      "Epoch: 27 \tTraining Loss: 0.011169 \tTraining Acc: 93.31%%\n",
      "Epoch: 28 \tTraining Loss: 0.011126 \tTraining Acc: 93.36%%\n"
     ]
    }
   ],
   "source": [
    "# number of epochs to train the model\n",
    "n_epochs = 30  # suggest training between 20-50 epochs\n",
    "\n",
    "model_2.train() # prep model for training\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # monitor training loss\n",
    "    train_loss = 0.0\n",
    "    total_correct = 0\n",
    "    \n",
    "    for data, target in train_loader:\n",
    "        \n",
    "        y_pred_2 = model_2(data)\n",
    "        loss_2 = loss_function_2(y_pred_2,target)\n",
    "        optimizer_2.zero_grad()\n",
    "        loss_2.backward()\n",
    "        optimizer_2.step()\n",
    "        \n",
    "        train_loss += loss_2.item()\n",
    "        _,pred_2 = torch.max(y_pred_2.data,1)\n",
    "        total_correct += torch.sum(pred_2 == target.data).item()\n",
    "        \n",
    "    # print training statistics \n",
    "    # calculate average loss and accuracy over an epoch\n",
    "    train_loss = train_loss / len(train_loader.dataset)\n",
    "    train_acc = 100. * total_correct / len(train_loader.dataset)\n",
    "    \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tTraining Acc: {:.2f}%%'.format(\n",
    "        epoch+1, \n",
    "        train_loss,\n",
    "        train_acc\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Test the Trained Network (20 marks)\n",
    "\n",
    "Test the performance of trained models on test data. Except the total test accuracy, you should calculate the accuracy for each class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test model_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\anaconda3\\envs\\pytorch\\lib\\site-packages\\ipykernel_launcher.py:19: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.012647\n",
      "\n",
      "Test Accuracy of class 0: 98.06%\n",
      "Test Accuracy of class 1: 97.53%\n",
      "Test Accuracy of class 2: 90.12%\n",
      "Test Accuracy of class 3: 91.68%\n",
      "Test Accuracy of class 4: 93.89%\n",
      "Test Accuracy of class 5: 88.00%\n",
      "Test Accuracy of class 6: 94.99%\n",
      "Test Accuracy of class 7: 92.80%\n",
      "Test Accuracy of class 8: 89.53%\n",
      "Test Accuracy of class 9: 91.77%\n",
      "\n",
      "Test Accuracy (Overall): 92.93%\n"
     ]
    }
   ],
   "source": [
    "# initialize lists to monitor test loss and accuracy\n",
    "test_loss = 0.0\n",
    "class_correct = list(0. for i in range(10))\n",
    "class_total = list(0. for i in range(10))\n",
    "\n",
    "model_1.eval() # prep model for *evaluation*\n",
    "\n",
    "for data, target in test_loader:\n",
    "    \n",
    "    # implement your code here\n",
    "    y_pred = model_1(data)\n",
    "    loss = loss_function(y_pred,target)\n",
    "\n",
    "    test_loss += loss.item()\n",
    "    _,pred = torch.max(y_pred.data,1)\n",
    "    for label in range(10):\n",
    "        # the list of number of correctly classified samples of each class of this batch. label is the index.\n",
    "        for i in range(20):\n",
    "            if target[i] == label and target[i] == pred[i]:\n",
    "                class_correct[label] += 1\n",
    "            if target[i] == label:\n",
    "                class_total[label] += 1\n",
    "#         print(label,class_correct[label])\n",
    "        # the list of total number of samples of each class of this batch. label is the index.\n",
    "#         class_total[label] += torch.sum(torch.eq(target.data, label)).item()\n",
    "#         print(label,class_total[label])\n",
    "\n",
    "# calculate and print avg test loss\n",
    "test_loss = test_loss / len(test_loader.dataset)\n",
    "print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
    "\n",
    "for i in range(10):\n",
    "    if class_total[i] > 0:\n",
    "        print('Test Accuracy of class %d: %.2f%%' % (i, 100 * class_correct[i] / class_total[i]))\n",
    "    else:\n",
    "        print('Test Accuracy of class %d: N/A (no training examples)' % (i))\n",
    "\n",
    "print('\\nTest Accuracy (Overall): %.2f%%' % (100. * np.sum(class_correct) / np.sum(class_total)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test model_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize lists to monitor test loss and accuracy\n",
    "test_loss = 0.0\n",
    "class_correct = list(0. for i in range(10))\n",
    "class_total = list(0. for i in range(10))\n",
    "\n",
    "model_2.eval() # prep model for *evaluation*\n",
    "\n",
    "for data, target in test_loader:\n",
    "    \n",
    "    # implement your code here\n",
    "    y_pred_2 = model_2(data)\n",
    "    loss_2 = loss_function(y_pred_2,target)\n",
    "\n",
    "    \n",
    "    test_loss += loss_2.item()\n",
    "    _,pred_2 = torch.max(y_pred_2.data,1)\n",
    "    for label in range(10):\n",
    "        # the list of number of correctly classified samples of each class of this batch. label is the index.\n",
    "        for i in range(20):\n",
    "            if target[i] == label and target[i] == pred_2[i]:\n",
    "                class_correct[label] += 1\n",
    "            if target[i] == label:\n",
    "                class_total[label] += 1\n",
    "# calculate and print avg test loss\n",
    "test_loss = test_loss / len(test_loader.dataset)\n",
    "print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
    "\n",
    "for i in range(10):\n",
    "    if class_total[i] > 0:\n",
    "        print('Test Accuracy of class %d: %.2f%%' % (i, 100 * class_correct[i] / class_total[i]))\n",
    "    else:\n",
    "        print('Test Accuracy of class %d: N/A (no training examples)' % (i))\n",
    "\n",
    "print('\\nTest Accuracy (Overall): %.2f%%' % (100. * np.sum(class_correct) / np.sum(class_total)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Analyze Your Result (20 marks)\n",
    "Compare the performance of your models with the following analysis. Both English and Chinese answers are acceptable.\n",
    "1. Does your vanilla MLP overfit to the training data? (5 marks)\n",
    "\n",
    "Answer:\n",
    "\n",
    "2. If yes, how do you observe it? If no, why? (5 marks)\n",
    "\n",
    "Answer:\n",
    "\n",
    "3. Is regularized model help prevent overfitting? (5 marks)\n",
    "\n",
    "Answer:\n",
    "\n",
    "4. Generally compare the performance of two models. (5 marks)\n",
    "\n",
    "Answer:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
